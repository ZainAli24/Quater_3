{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/Quater_3/blob/main/Q3class2_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9CQ_Kr9gw5p"
      },
      "source": [
        "##1) **AI Agents Development Stack Overview**  \n",
        "\n",
        "AI agents ka system 4 layers mein divide kiya ja sakta hai:  \n",
        "\n",
        "1. **Model Serving:**  \n",
        "   - Large Language Models (LLMs) jaise OpenAI aur Anthropic se kaam karte hain.  \n",
        "   - Ye models human-like text process aur generate karte hain.  \n",
        "\n",
        "2. **Storage:**  \n",
        "   - Agents ko apni information (jaise conversation history) store aur retrieve karni hoti hai.  \n",
        "   - Iske liye Vector Databases jaise Chroma, Weaviate, aur Pinecone use hote hain.  \n",
        "\n",
        "3. **Tools & Libraries:**  \n",
        "   - Agents text generation ke alawa aur tasks ke liye tools ka use karte hain.  \n",
        "   - LangChain jaisi libraries tools ko connect karne ka infrastructure deti hain.  \n",
        "\n",
        "4. **Agent Frameworks:**  \n",
        "   - Ye frameworks agents ke kaam ko manage karte hain (state, tools, interaction).  \n",
        "   - Examples: **CrewAI** (simple teamwork) aur **LangGraph** (advanced workflows).  \n",
        "\n",
        "\n",
        "\n",
        "## ~) **CrewAI and LangGraph**\n",
        "\n",
        "### **CrewAI: Teamwork Wale AI Agents**  \n",
        "Imagine ek team hai jisme har member ka ek specific role hai, jaise cricket team mein batsman, bowler, fielder hote hain. **CrewAI** ka kaam hai aise AI agents banane ka jo ek team ki tarah kaam karein, har agent apna role play karta hai.  \n",
        "- **Role-based ka matlab:** Har agent ka apna kaam fix hota hai.  \n",
        "- **User-friendly:** Isko use karna asaan hai, beginner-friendly hai.  \n",
        "- **LangChain ke saath:** Ye ek powerful tool (LangChain) ke saath kaam karta hai, jo AI models aur tools ke liye ek library jaisa hai.  \n",
        "- **Tracking tools:** Har agent kya progress kar raha hai, wo dekhne ke liye reporting tools diye gaye hain.  \n",
        "\n",
        "### **LangGraph: Workflow Ka Map**  \n",
        "Socho tumhare kaam ka ek flow chart hai, jisme ek kaam khatam hone ke baad agla kaam hota hai. **LangGraph** isi tarah kaam karta hai, har kaam ko ek \"node\" (point) ki tarah treat karta hai aur inka connection samajhta hai.  \n",
        "- **Graph-based:** Kaam ko ek map ki tarah organize karta hai.  \n",
        "- **Multi-agent:** Agar ek se zyada AI agents ho, to wo ek doosre ke saath interact kar sakte hain.  \n",
        "- **Customization:** Tum apne workflow ko apni marzi se design kar sakte ho, complex cheezein bhi manage kar sakte ho.  \n",
        "\n",
        "### **Comparison: CrewAI vs LangGraph**  \n",
        "- **Use Karne Mein Asaani:** CrewAI simple aur beginner-friendly hai. LangGraph thoda advanced aur technical hai, graph samajhne ki zarurat hai.  \n",
        "- **Flexibility:** LangGraph zyada customize ho sakta hai, CrewAI ek fixed (opinionated) system jaisa hai.  \n",
        "- **Tools ka Access:** Dono hi LangChain ke tools aur models ka use karte hain, to features mein similar hain.  \n",
        "\n",
        "### **Conclusion**  \n",
        "Agar tumhe ek simple AI teamwork chahiye, to **CrewAI** best hai. Agar tumhe zyada control aur customization chahiye, to **LangGraph** choose karo. Dono frameworks apne-apne tarike se AI systems ko powerful aur organized banate hain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcDF6F5zFHm8"
      },
      "source": [
        "#2) CPU vs GPU Comparison\n",
        "\n",
        "| **Feature**       | **CPU**                                      | **GPU**                                         |\n",
        "|--------------------|----------------------------------------------|------------------------------------------------|\n",
        "| **Purpose**        | General-purpose kaam (daily tasks, apps, etc.) | Specialized kaam (graphics, machine learning, etc.) |\n",
        "| **Cores**          | Kam cores (2-16)                            | Bahut zyada cores (thousands)                 |\n",
        "| **Task Handling**  | Ek-ek kaam sequentially karta hai            | Ek saath multiple kaam karta hai (parallel tasks) |\n",
        "| **Applications**   | Apps, operating systems, basic computing     | Graphics, scientific simulations, machine learning |\n",
        "\n",
        "\n",
        "# __Summary__:\n",
        "**CPU**: General computing ke liye best (ek-ek kaam fast karne ke liye).\n",
        "\n",
        "**GPU**: Parallel tasks ke liye best (machine learning aur graphics jese kaam).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Im Note** :\n",
        "- Nvidia [90% Market Share]\n",
        "> (G Force, RTX 4090 4nm)\n",
        "\n",
        " > Blackwell(3nm)\n",
        "\n",
        "- Semi Conductor Chips - **Fabricated by TSMC**\n",
        "- AMD - Fabricated by TSMC\n",
        "\n",
        "- Apple - A18(3nm) - **Fabricated by TSMC**\n",
        "\n",
        "- Samsung - Qualcomm SnapDragon8 Gen3 (4nm) - **Fabricated by TSMC**\n",
        "\n",
        "- Intel - **Fabricated by self**"
      ],
      "metadata": {
        "id": "-LWpHpr6L7HF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFRVRDwl2dk"
      },
      "source": [
        "#3) **Compound AI Systems**:\n",
        "\n",
        "Compound AI Systems ek nayi tarah ka AI approach hai jo ek model ke bajaye, multiple models, tools, aur techniques ko combine karte hain. Isse complex problems ko zyada efficiently solve kiya ja sakta hai.  \n",
        "\n",
        "---\n",
        "\n",
        "### Key Points  \n",
        "\n",
        "1. **Kya Hain Compound AI Systems?**  \n",
        "   - Ye multiple AI models aur tools ko ek system mein mila kar kaam karte hain.  \n",
        "   - Ek standalone model ke bajaye, har part apni strength ko use karta hai.  \n",
        "\n",
        "2. **Khasiyat (Features)**  \n",
        "   - **Dynamic aur Flexible:** Bahar ke resources jaise databases aur tools ko use karte hain.  \n",
        "   - **Zyada Reliable Results:** Har component ka output control aur check kiya ja sakta hai.  \n",
        "   - **Better Performance:** Har specialized part mil kar kaam karta hai.  \n",
        "\n",
        "3. **Agentic AI Systems**  \n",
        "   - Ye ek type ka Compound AI System hai jisme AI agents apne decisions aur planning khud karte hain.  \n",
        "   - LLMs (Large Language Models) ka use karke kaam ko bina detailed instructions ke execute karte hain.  \n",
        "\n",
        "4. **Fark Compound aur Agentic AI Systems Mein**  \n",
        "   - **Compound AI Systems:** Alag-alag models combine hote hain par predefined logic follow karte hain.  \n",
        "   - **Agentic AI Systems:** Zyada autonomy hoti hai aur agents apne decisions khud lete hain.  \n",
        "\n",
        "5. **Applications (Kaha Use Hote Hain)**  \n",
        "   - **Chatbots:** Natural language ko samajhne aur correct jawab dene ke liye databases ke saath kaam karte hain.  \n",
        "   - **Autonomous Agents:** Tasks jaise email manage karna ya meetings schedule karna khud karte hain.  \n",
        "\n",
        "6. **Summary**  \n",
        "   - Compound AI Systems: Ye alag-alag AI tools ka combined use karke tasks solve karte hain. Har tool apne specific kaam karta hai, lekin predefined logic ke under.\n",
        "\n",
        "   - Agentic AI Systems: Is framework mein autonomy hoti hai, jisse AI apne decisions khud le sakta hai aur independently planning aur actions execute karta hai.  \n",
        "\n",
        "---\n",
        "\n",
        "Ye systems AI ko zyada powerful aur adaptable banate hain, jisse complex real-world problems solve ki ja sakti hain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ELEm6Z9ytLT"
      },
      "source": [
        "# **LangChain** :  use of langChain for Hello Word with google gemini model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LBD2P3z15Qgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5260e109-f7aa-490a-fb10-7d6c8781b6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0I-AZ2ZS6OBp"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h6PI1VyB6suT"
      },
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "    api_key=api_key,\n",
        "    model='gemini-1.5-flash',\n",
        "    temperature=0.9\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgGMXV-P7opz",
        "outputId": "92e495b2-1342-43d2-f5b6-4985cc7e2369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='There\\'s no single \"best\" Mercedes-Benz model, as the ideal choice depends heavily on individual needs and preferences.  However, some models consistently receive high praise and are often considered among the best, depending on the category:\\n\\n* **For Luxury Sedans:** The S-Class is frequently cited as a benchmark for luxury and technology.  It represents the pinnacle of Mercedes engineering and comfort.\\n\\n* **For Sporty Sedans/SUVs:** The AMG GT 4-Door Coupe and the AMG GLE 63 S blend high performance with luxury.  These models are for those who want the best of both worlds.\\n\\n* **For Luxury SUVs:** The GLS-Class offers spaciousness, luxury, and capability.  The G-Class is iconic and highly capable off-road, though less practical for everyday use.\\n\\n* **For Compact Luxury:** The C-Class is a popular choice, offering a balance of luxury, practicality, and a more affordable price point than the S-Class.\\n\\nUltimately, the \"best\" Mercedes model for *you* depends on factors like:\\n\\n* **Budget:** Prices vary dramatically across the model range.\\n* **Size and Space:**  Do you need a compact car, a large sedan, or a spacious SUV?\\n* **Performance:** Are you looking for a comfortable cruiser or a high-performance vehicle?\\n* **Features:**  Specific technology and comfort features are important to consider.\\n\\nResearching specific model years within these categories is also vital.  A particular year might have issues that others don\\'t.  Reading reviews from reputable sources is highly recommended before making a purchase.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-726f6db1-137f-4c73-aaaf-c1e2fcf89791-0' usage_metadata={'input_tokens': 14, 'output_tokens': 337, 'total_tokens': 351, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "print(model.invoke(\"which spcecific model of mercedes is one of the best model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOfApf2XA2IQ",
        "outputId": "2692bd21-d1b8-49ad-df6b-1d7a27ddeeb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Okay Zain, since you were born in 2004, you are currently **19 years old**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-dbd8fad0-30cd-4e7c-a967-545bbaf9af22-0', usage_metadata={'input_tokens': 23, 'output_tokens': 24, 'total_tokens': 47, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model=ChatGoogleGenerativeAI(\n",
        "    api_key=api_key,\n",
        "    model='gemini-2.0-flash-exp'\n",
        ")\n",
        "model.invoke(\"My Name is Zain , I was born in 2004 please tell me my age with my name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kon0By8z1uc9"
      },
      "source": [
        "# **Project** **1**: LangChain Hello World Project\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qJyyOWPK16-H"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ByCWQ_L_2cQL"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "50pXtthP3x4C"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ILNHHm6N3Mm5"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=api_key,\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJc_xhEGB-jV",
        "outputId": "d32841b5-cf20-4a01-c104-6f18dd65b745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-acf78ba209ca>:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=template)\n",
            "<ipython-input-10-acf78ba209ca>:8: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\" : question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's explore some logic-building techniques in Python, categorized for clarity:\n",
            "\n",
            "**I. Foundational Concepts:**\n",
            "\n",
            "* **Decomposition:** Break down complex problems into smaller, more manageable subproblems.  This often leads to creating functions or methods that handle specific parts of the overall logic.  Example: Instead of writing one giant function to process an image, create separate functions for image loading, filtering, edge detection, and saving.\n",
            "\n",
            "* **Abstraction:** Hide unnecessary details and focus on essential information.  Use data structures (lists, dictionaries, classes) to represent data effectively, and functions to encapsulate logic. This improves code readability and maintainability.  Example:  Don't worry about the *how* of sorting a list; just use the `list.sort()` method and focus on what you're sorting.\n",
            "\n",
            "* **Pattern Recognition:** Identify recurring patterns in your problem or code.  This can lead to using loops, recursion, or design patterns (like the Template Method or Strategy pattern) to solve similar problems more efficiently. Example: If you're repeatedly checking for conditions, consider using a `for` loop or a conditional expression.\n",
            "\n",
            "**II. Control Flow Techniques:**\n",
            "\n",
            "* **Conditional Statements (if, elif, else):**  Master the use of these for decision-making based on different conditions.  Practice nested `if` statements and consider using boolean logic operators (`and`, `or`, `not`) effectively.\n",
            "\n",
            "* **Loops (for, while):** Learn to iterate over sequences (lists, strings, tuples) using `for` loops and use `while` loops for situations where the number of iterations isn't known beforehand. Practice using `break` and `continue` statements to control loop execution.\n",
            "\n",
            "* **Iteration vs. Recursion:** Understand the difference between iterative and recursive approaches. Recursion is elegant for problems that can be broken down into smaller, self-similar subproblems, but be mindful of stack overflow issues for deeply nested recursion.\n",
            "\n",
            "**III. Data Structures and Algorithms:**\n",
            "\n",
            "* **Lists & Tuples:** Understand their differences (mutability vs. immutability) and how to use them effectively for storing and manipulating data.\n",
            "\n",
            "* **Dictionaries & Sets:**  Dictionaries provide efficient key-value lookups; sets offer fast membership testing. Learn when to use each for optimal performance.\n",
            "\n",
            "* **Algorithm Design:**  Familiarize yourself with common algorithms like searching (linear, binary), sorting (bubble sort, merge sort, quicksort), and graph traversal (BFS, DFS).  Understanding these building blocks will help you solve more complex problems efficiently.\n",
            "\n",
            "\n",
            "**IV.  Debugging and Testing:**\n",
            "\n",
            "* **Print Statements (for debugging):**  Strategic placement of `print()` statements can help you understand the flow of your program and identify errors.\n",
            "\n",
            "* **Logging:** For larger projects, use the `logging` module to record program events, making debugging easier.\n",
            "\n",
            "* **Unit Testing:** Write unit tests to verify that individual components of your code work correctly.  Python's `unittest` module is a valuable tool for this.\n",
            "\n",
            "**V.  Practice Problems:**\n",
            "\n",
            "* **Start Simple:** Begin with basic problems like calculating averages, finding maximum values, or checking for palindromes.\n",
            "\n",
            "* **Work Your Way Up:** Gradually increase the complexity, tackling problems like sorting algorithms, string manipulation, or working with files.\n",
            "\n",
            "* **Use Online Resources:** Websites like HackerRank, LeetCode, and Codewars offer a wealth of coding challenges to hone your logic-building skills.\n",
            "\n",
            "**Example:  Finding the largest number in a list**\n",
            "\n",
            "A simple problem demonstrating several techniques:\n",
            "\n",
            "```python\n",
            "def find_largest(numbers):\n",
            "    \"\"\"Finds the largest number in a list using a loop.\"\"\"\n",
            "    if not numbers:  # Handle empty list case\n",
            "        return None\n",
            "\n",
            "    largest = numbers[0]  # Initialize with the first element\n",
            "    for number in numbers:\n",
            "        if number > largest:\n",
            "            largest = number\n",
            "    return largest\n",
            "\n",
            "numbers = [1, 5, 2, 9, 3]\n",
            "largest_number = find_largest(numbers)\n",
            "print(f\"The largest number is: {largest_number}\")\n",
            "\n",
            "\n",
            "#Alternative using the max function (abstraction)\n",
            "largest_number_alt = max(numbers)\n",
            "print(f\"The largest number (using max function): {largest_number_alt}\")\n",
            "```\n",
            "\n",
            "Remember, consistent practice and a methodical approach are key to improving your logic-building skills in Python (or any programming language).  Don't be afraid to break down problems, experiment, and learn from your mistakes.\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a powerfull assistant in coding suggestions. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=template)\n",
        "question = \"Give me some suggestion for logic building in python\"\n",
        "response = chain.run({\"question\" : question})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcADEldfPUgt"
      },
      "source": [
        "#1) **Model** : Coding Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlZUTcYuHGXZ",
        "outputId": "e3795064-67f1-4b40-8115-8278c77bfe29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only For coding question.\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a powerfull assistant in coding suggestions, if user question is another topic please stop execution and say Only For coding question. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=template)\n",
        "question = \"Give me some suggestion for car driving\"\n",
        "response = chain.run({\"question\" : question})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txtOGyWdPwEm"
      },
      "source": [
        "#2) **Model** : Language translator Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P0jjpclNyTi",
        "outputId": "e3e220c0-a01e-4970-8a30-0004d1bf9788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "آپ میرے سب سے اچھے دوست ہیں، کیا آپ میری مدد کر سکتے ہیں؟  (Aap mere sab se ache dost hain, kya aap meri madad kar sakte hain?)\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(\n",
        "    input_variables=[\"text\" , \"language\"],\n",
        "    template=\"You are Powerfull language translater. Translate the following text to {language}:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm  , prompt=template)\n",
        "text = \"You are my best friend , Can you help me Please\"\n",
        "language = \"Urdu\"\n",
        "response = chain.run({\"text\":text , \"language\":language})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBNMJVDQIlp"
      },
      "source": [
        "#3) **Model** :  Python Quiz Maker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoVqsyrUQTUx",
        "outputId": "d6da592c-8ea3-40b9-9a1d-05d49d780b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Python Functions Quiz\n",
            "\n",
            "**Instructions:** Choose the best answer for each multiple-choice question.\n",
            "\n",
            "**1. What is the purpose of a function in Python?**\n",
            "\n",
            "a) To perform a specific task or a series of tasks.\n",
            "b) To declare a variable.\n",
            "c) To control the flow of a program using loops.\n",
            "d) To define a data structure.\n",
            "\n",
            "**2. Which keyword is used to define a function in Python?**\n",
            "\n",
            "a) `class`\n",
            "b) `def`\n",
            "c) `function`\n",
            "d) `procedure`\n",
            "\n",
            "**3. What is the correct way to call a function named `my_function` that takes two arguments, `x` and `y`?**\n",
            "\n",
            "a) `my_function x y`\n",
            "b) `my_function(x, y)`\n",
            "c) `my_function{x, y}`\n",
            "d) `my_function[x, y]`\n",
            "\n",
            "**4.  A function that does not return a value implicitly returns:**\n",
            "\n",
            "a) `None`\n",
            "b) `0`\n",
            "c) An error\n",
            "d) The last value assigned within the function\n",
            "\n",
            "**5. What does the `return` statement do in a Python function?**\n",
            "\n",
            "a) Prints the result to the console.\n",
            "b) Exits the function and optionally sends a value back to the caller.\n",
            "c) Creates a loop within the function.\n",
            "d) Defines a variable within the function.\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "**Answer Key:**\n",
            "\n",
            "1.  a) To perform a specific task or a series of tasks.\n",
            "2.  b) `def`\n",
            "3.  b) `my_function(x, y)`\n",
            "4.  a) `None`\n",
            "5.  b) Exits the function and optionally sends a value back to the caller.\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are teacher of Python Quiz Maker , Create a quiz with 5 questions about the topic:\\n\\n{topic} \"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm , prompt=template)\n",
        "topic = \"functions\"\n",
        "response = chain.run({\"topic\":topic})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz5xlfjeTkxX"
      },
      "source": [
        "# 4) **Model** :  Customer Support Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsuHwUVXT2xz",
        "outputId": "61cc54c5-852d-48d4-e283-f90453868878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for your question.  While I appreciate you reaching out, my expertise lies in web development and I'm not able to provide guidance on which car is best for drifting.  I'd recommend searching online car forums or automotive review sites for information on that topic.  If you have any questions related to web development, such as website design, hosting, programming languages, or any other web-related issue, please don't hesitate to ask. I'm happy to help with those.\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template = \"\"\"\n",
        "You are a customer support agent specializing in the web development field.\n",
        "Please respond to the following query in a polite and professional manner:\n",
        "{query}\n",
        "If the query is unrelated to web development, politely inform the user that you are here to assist only with web development-related questions.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm , prompt=template)\n",
        "query = \"Which Car is best for drifting\"\n",
        "response = chain.run({\"query\":query})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH2iLY7A2dla"
      },
      "source": [
        "  # **Use LangChain’s memory feature** : to store previous conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p15yersW3EKs"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNQ9g7Rq36r5",
        "outputId": "6eb5eb93-0387-4ff2-a409-44245d51a7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-a2a35039e5eb>:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-16-a2a35039e5eb>:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Okay, let's talk about LangChain! I'm happy to dive into the details.\n",
            "\n",
            "LangChain is essentially a framework designed to make it easier to build applications powered by large language models (LLMs). Think of it as a toolkit or a library that provides a whole bunch of pre-built components and abstractions that handle a lot of the complexities involved in working with LLMs. Instead of having to write all the code from scratch, you can use LangChain to quickly put together sophisticated applications.\n",
            "\n",
            "Here's a breakdown of some of the key aspects:\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **LLM Integration:** At its core, LangChain provides a standardized interface for interacting with various LLMs. This means you can swap out one LLM for another relatively easily, without needing to rewrite significant portions of your code. It supports popular models from providers like OpenAI, Anthropic, Hugging Face, and more.\n",
            "\n",
            "*   **Chains:** This is where the \"Chain\" part of LangChain comes in. A chain is a sequence of calls to LLMs or other utilities. These chains can be simple, like prompting an LLM to generate a response, or complex, involving multiple LLM calls, data transformations, and external tool usage. This allows you to build sophisticated workflows.\n",
            "\n",
            "*   **Agents:** Agents are powerful because they can make decisions about which actions to take based on the current context. They use an LLM to decide the optimal next step in a process, which could include calling other tools, manipulating data, or making an API request. This allows for much more dynamic and adaptable applications.\n",
            "\n",
            "*   **Prompt Engineering Support:** LangChain helps with crafting effective prompts for LLMs. It provides tools for creating prompt templates, managing variable inputs, and formatting output. This is crucial because the effectiveness of an LLM often depends on the quality of the prompt.\n",
            "\n",
            "*   **Data Connections:** Many applications require access to external data. LangChain has integrations for connecting to various data sources like databases, files, APIs, and web pages. This allows your LLM applications to have access to real-world information.\n",
            "\n",
            "*   **Memory:** LLMs are generally stateless, meaning they don't remember previous interactions. LangChain provides mechanisms to add memory to your applications, allowing you to maintain context across multiple turns in a conversation or over a series of tasks.\n",
            "\n",
            "**Why is it useful?**\n",
            "\n",
            "*   **Reduced Development Time:** By providing pre-built components and abstractions, LangChain significantly reduces the time it takes to build LLM applications.\n",
            "\n",
            "*   **Improved Application Complexity:** It enables the creation of more sophisticated applications that can perform tasks such as summarization, question answering, code generation, and even more complex workflows.\n",
            "\n",
            "*   **Flexibility:** It's flexible in that it doesn't lock you into using only a single type of LLM or data source. It supports multiple providers and many different types of integrations.\n",
            "\n",
            "*   **Active Community:** LangChain has an active open-source community, which means you get access to community support, updates, and new features.\n",
            "\n",
            "**Example Scenarios**\n",
            "\n",
            "*   **Chatbots:** Creating chatbots that can understand context and maintain a conversation over multiple turns.\n",
            "*   **Document Analysis:** Building applications that can analyze documents, summarize them, or extract key information.\n",
            "*   **Data-Driven Applications:** Connecting LLMs to databases to answer questions based on real-world data.\n",
            "*   **Code Generation:** Using LLMs to generate code snippets based on requirements.\n",
            "*   **Personalized Recommendations:** Creating systems that can provide personalized recommendations based on user data.\n",
            "\n",
            "**In short, LangChain is an incredibly versatile tool for anyone looking to build applications powered by the latest LLMs. It simplifies the development process and allows you to create more powerful and sophisticated applications.**\n",
            "\n",
            "Do you have any specific questions about any part of LangChain? I'm happy to go into more detail on any of these aspects, or anything else that comes to mind!\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash-exp\",\n",
        "    api_key=api_key,\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "response1 = conversation.run(\"What is LangChain?\")\n",
        "print(\"Answer:\", response1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6MF7DMc6Rr4",
        "outputId": "4fe96e54-04b9-4c35-f9d5-7f4837af446e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: That's an excellent question! Memory is a crucial aspect of building conversational or multi-step applications with LLMs, and LangChain provides several ways to incorporate it. Let's delve into the details of how LangChain handles memory:\n",
            "\n",
            "**The Challenge of Statelessness:**\n",
            "\n",
            "As I mentioned before, LLMs are inherently stateless. This means that each interaction with an LLM is treated as an isolated event. By default, the LLM has no recollection of previous conversations or steps taken. This poses a problem when you want to build applications that require context or the ability to remember past interactions.\n",
            "\n",
            "**LangChain's Memory Solutions:**\n",
            "\n",
            "LangChain offers various memory modules that allow you to add \"memory\" to your LLM applications. These modules essentially store and retrieve information across interactions. Here are some key memory concepts and implementations in LangChain:\n",
            "\n",
            "1.  **Conversation Buffer Memory:**\n",
            "    *   **How it works:** This is one of the simplest forms of memory. It stores the entire conversation history (both user input and AI output) in a list.\n",
            "    *   **Use Case:** Good for maintaining a short to medium-length conversational context.\n",
            "    *   **Mechanism:** Each new interaction appends to the conversation history list. Before prompting the LLM, the entire history is sent as part of the prompt.\n",
            "    *   **Pros:** Easy to use, provides a complete record of the conversation.\n",
            "    *   **Cons:** Can become very large, leading to longer prompts, higher costs, and potentially slower responses as the conversation grows. It has limitations on how much context can be sent, because LLMs have token limits.\n",
            "\n",
            "2.  **Conversation Buffer Window Memory:**\n",
            "    *   **How it works:** Similar to the Conversation Buffer Memory, but instead of storing the entire history, it stores only the most recent `k` interactions.\n",
            "    *   **Use Case:** Effective for keeping a limited context without making the prompt too long.\n",
            "    *   **Mechanism:** When a new interaction occurs, the oldest interaction is removed from the buffer if the buffer has reached its size limit.\n",
            "    *   **Pros:** Avoids large prompts, more efficient than Buffer Memory, balances memory with cost.\n",
            "    *   **Cons:** Older interactions are completely forgotten.\n",
            "\n",
            "3.  **Conversation Summary Memory:**\n",
            "    *   **How it works:** Instead of storing the raw conversation history, this memory module summarizes the conversation into a concise summary.\n",
            "    *   **Use Case:** Effective for long-running conversations or when you want to retain high-level context without the full details.\n",
            "    *   **Mechanism:** After each interaction, the memory module uses an LLM to summarize the conversation up to that point, then it saves the summarized context as the memory.\n",
            "    *   **Pros:** Significantly reduces the size of the prompt, good for long conversations, retains key information.\n",
            "    *   **Cons:** Lossy; some details may be lost in the summarization process, and a separate LLM call is needed to create the summary, which can increase costs and latency.\n",
            "\n",
            "4.  **Conversation Summary Buffer Memory:**\n",
            "    *   **How it works:** A hybrid approach. It keeps recent conversations in a buffer like Conversation Buffer Window Memory, and also condenses the older parts of the conversation into a summary.\n",
            "    *   **Use Case:** Effective for managing long conversations while keeping recent details available.\n",
            "    *   **Mechanism:** It maintains a buffer of the latest `k` interactions, and periodically summarizes the older conversation into a summary, then prepends the summary to the buffer.\n",
            "    *   **Pros:** Balances the need for detailed recent context with a higher-level view of the longer conversation.\n",
            "    *   **Cons:** More complex to configure than simpler memory types.\n",
            "\n",
            "5. **Entity Memory:**\n",
            "    * **How it works:** Instead of simply remembering the conversation as a whole, it keeps track of specific entities (like names, places, things, etc.) and their attributes. It extracts relevant entities from each interaction.\n",
            "    * **Use Case:** Very good when you need the LLM to remember details about specific objects or people.\n",
            "    * **Mechanism:** The memory extracts new entities using an LLM, updates the internal state of existing entities, then uses this updated state in the next prompt.\n",
            "    * **Pros:** Allows the LLM to know specific details rather than just a general conversation.\n",
            "    * **Cons:** More complicated to implement correctly and can have a higher operational cost.\n",
            "\n",
            "6.  **Vector Store Memory:**\n",
            "    *   **How it works:** Stores past interactions or relevant information in a vector database (like Chroma, FAISS, etc.).\n",
            "    *   **Use Case:** Powerful for retrieving contextual information that is not strictly part of the immediate conversation, or when you want semantic similarity rather than exact matches.\n",
            "    *   **Mechanism:** Each interaction or document is embedded into a vector, and when a new prompt is received, the most semantically similar vectors are retrieved from the vector database and included in the prompt.\n",
            "    *   **Pros:** Enables retrieval of relevant information, even if the exact wording was not used before.\n",
            "    *   **Cons:** Requires setup of a vector database and more complex setup.\n",
            "\n",
            "**How to Use Memory in LangChain:**\n",
            "\n",
            "1.  **Choosing a Memory Type:** Select the memory module that best fits your application's needs.\n",
            "2.  **Instantiating the Memory:** Create an instance of the selected memory class.\n",
            "3.  **Integrating with a Chain:** Pass the memory instance to your `Chain` object. The Chain will automatically handle saving and loading the memory before and after each interaction.\n",
            "4.  **Managing the Memory:** The Chain object interacts with the memory module behind the scenes, adding new turns to it, and retrieving the saved context to be added to the prompt.\n",
            "\n",
            "**Example (Conceptual, using ConversationBufferMemory):**\n",
            "\n",
            "```python\n",
            "from langchain.memory import ConversationBufferMemory\n",
            "from langchain.chains import LLMChain\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "# Initialize LLM and memory\n",
            "llm = OpenAI(temperature=0.7)\n",
            "memory = ConversationBufferMemory()\n",
            "\n",
            "# Define Prompt\n",
            "template = \"\"\"You are a helpful chatbot. Respond in a friendly manner.\n",
            "{history}\n",
            "User: {input}\n",
            "Chatbot:\"\"\"\n",
            "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
            "\n",
            "\n",
            "# Create LLMChain with Memory\n",
            "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
            "\n",
            "\n",
            "# First interaction\n",
            "output = chain.run(input=\"Hello, how are you?\")\n",
            "print(output)\n",
            "\n",
            "# Second interaction (memory is maintained)\n",
            "output = chain.run(input=\"What did you say before?\")\n",
            "print(output)\n",
            "```\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "*   LangChain memory modules provide a way to maintain conversational state for LLMs.\n",
            "*   Different types of memory offer different trade-offs between context, cost, and complexity.\n",
            "*   Memory is integrated seamlessly into LangChain's `Chain` system.\n",
            "*   Choosing the right memory type is critical for the performance and functionality of your application.\n",
            "\n",
            "I hope this detailed explanation of memory in LangChain is helpful! Do you have any further questions about specific memory types, their implementation, or use cases? I'm happy to provide more specific examples or information.\n"
          ]
        }
      ],
      "source": [
        "response2 = conversation.run(\"How can it be used with memory?\")\n",
        "print(\"Answer:\", response2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lehbNrcS6hyF",
        "outputId": "bafa9c50-fdd0-4a06-98ee-2733f246b5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Okay, I can definitely synthesize the explanations of LangChain and its memory capabilities into a combined overview. Let's think of it as a holistic picture of how LangChain, with its memory features, helps in building more sophisticated LLM applications:\n",
            "\n",
            "**LangChain: The Foundation for LLM Applications**\n",
            "\n",
            "At its core, LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It acts as a powerful toolkit, providing pre-built components and abstractions that handle many of the complexities involved in working with LLMs. Instead of writing everything from scratch, you can leverage LangChain's features to rapidly build complex applications.\n",
            "\n",
            "Here are the key concepts:\n",
            "\n",
            "*   **LLM Integration:** LangChain offers a standardized way to interact with various LLMs (like OpenAI, Anthropic, Hugging Face), making it easy to swap models without significant code changes.\n",
            "*   **Chains:** The \"Chain\" in LangChain refers to sequences of calls to LLMs or other utilities. These chains can be simple or complex, involving multiple LLM calls, data transformations, and external tool usage, enabling sophisticated workflows.\n",
            "*   **Agents:** Agents are dynamic decision-makers. They use an LLM to determine the optimal next step in a process, including calling tools or APIs, which enables adaptability in applications.\n",
            "*   **Prompt Engineering:** LangChain provides tools to create effective prompts, manage variables, and format outputs, which is vital for getting the best results from LLMs.\n",
            "*   **Data Connections:** LangChain allows connections to external data sources like databases, APIs, and files, giving LLM applications real-world data access.\n",
            "*   **Memory:** LLMs are stateless; they don't inherently remember previous interactions. LangChain offers various memory modules to maintain context across interactions, which is what we'll delve into in more detail below.\n",
            "\n",
            "**Why LangChain is Useful**\n",
            "\n",
            "*   **Reduced Development Time:** Pre-built components speed up application development.\n",
            "*   **Improved Application Complexity:** Enables creation of advanced applications for tasks like summarization, question answering, and more.\n",
            "*   **Flexibility:** Supports diverse LLMs and data sources.\n",
            "*   **Active Community:** Benefits from community support and continuous updates.\n",
            "\n",
            "**Memory: Adding Context to Stateless LLMs**\n",
            "\n",
            "Since LLMs are stateless, they don't inherently remember previous interactions. This is where LangChain's memory modules come into play. They provide mechanisms to store and retrieve information across multiple interactions, enabling applications to maintain context.\n",
            "\n",
            "Here's a breakdown of the different types of memory:\n",
            "\n",
            "1.  **Conversation Buffer Memory:**\n",
            "    *   **How:** Stores the entire conversation history in a list.\n",
            "    *   **Use:** Good for short to medium-length conversational context.\n",
            "    *   **Pros:** Simple to use, complete record of the conversation.\n",
            "    *   **Cons:** Can become very large, leading to longer prompts, higher costs, and slower responses.\n",
            "\n",
            "2.  **Conversation Buffer Window Memory:**\n",
            "    *   **How:** Stores only the most recent `k` interactions.\n",
            "    *   **Use:** Effective for limited context without overly long prompts.\n",
            "    *   **Pros:** Avoids large prompts, more efficient than Buffer Memory.\n",
            "    *   **Cons:** Older interactions are completely forgotten.\n",
            "\n",
            "3.  **Conversation Summary Memory:**\n",
            "    *   **How:** Summarizes the conversation into a concise summary using an LLM.\n",
            "    *   **Use:** Ideal for long-running conversations, keeping a high-level context.\n",
            "    *   **Pros:** Significantly reduces prompt size, retains key information.\n",
            "    *   **Cons:** Lossy; some details are lost in summarization, and it increases costs and latency due to the LLM summarization call.\n",
            "\n",
            "4.  **Conversation Summary Buffer Memory:**\n",
            "    *   **How:** Combines a recent interaction buffer with a summary of older interactions.\n",
            "    *   **Use:** Manages long conversations by balancing detailed recent context with a high-level overview.\n",
            "    *   **Pros:** Balances the need for detail and a summary view.\n",
            "    *   **Cons:** More complex to configure.\n",
            "\n",
            "5.  **Entity Memory:**\n",
            "    *   **How:** Tracks specific entities and their attributes from the conversation.\n",
            "    *   **Use:** Ideal for when the LLM needs to recall details about specific objects or people.\n",
            "    *   **Pros:** Enables remembering specific details rather than just a general conversation.\n",
            "    *   **Cons:** More complicated to implement correctly and higher operational costs.\n",
            "\n",
            "6.  **Vector Store Memory:**\n",
            "    *   **How:** Stores past interactions in a vector database for semantic similarity retrieval.\n",
            "    *   **Use:** Retrieves relevant information based on meaning, not just the exact wording.\n",
            "    *   **Pros:** Enables retrieval of relevant information, even if the wording was not used before.\n",
            "    *   **Cons:** Requires setting up a vector database and is more complex.\n",
            "\n",
            "**How LangChain Integrates Memory**\n",
            "\n",
            "1.  **Choosing a Memory Type:** Select the most appropriate memory module for your application.\n",
            "2.  **Instantiating:** Create an instance of the chosen memory class.\n",
            "3.  **Integrating with Chains:** Pass the memory instance to your `Chain` object. The Chain will automatically handle memory saving and retrieval.\n",
            "4.  **Managing Memory:** The Chain transparently manages adding to the memory and retrieving saved context to be included in the prompt to the LLM.\n",
            "\n",
            "**The Combined Picture: Putting It All Together**\n",
            "\n",
            "LangChain provides the tools to build the application, and memory modules make those applications more interactive and context-aware. With LangChain, you can create sophisticated applications, like:\n",
            "\n",
            "*   **Chatbots** that remember the conversation history.\n",
            "*   **Document Analysis tools** that can recall previous interactions while extracting information.\n",
            "*   **Personalized Recommendation systems** that can recall previous user interactions for better recommendations.\n",
            "*   **Complex workflows** that can remember previous steps and make decisions based on them.\n",
            "\n",
            "By combining LangChain's framework with its various memory modules, developers can build much more powerful and nuanced LLM-powered applications, making it a key resource in the world of AI application development.\n",
            "\n",
            "Do you feel that this combined explanation is clear and helpful? I'm happy to elaborate on any specific areas or provide examples if you'd like.\n"
          ]
        }
      ],
      "source": [
        "response3 = conversation.run(\"Can you explain both answers together?\")\n",
        "print(\"Answer:\", response3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmkAUK3Vv1Y7"
      },
      "source": [
        "###    Use Prompt_template with langchain memory feature(**ConversationBufferMemory**)\n",
        "\n",
        "*  **Key Point**:\n",
        "     Agar aapko simple chat banani hai, to ConversationChain best hai.\n",
        "     Agar aapko prompts ko customize karna hai aur model ko specific behavior sikhana hai, to LLMChain ke sath PromptTemplate use karein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uTbHsXK4wtX0"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R2ICPezxKBp",
        "outputId": "8c21b1c9-32be-42a9-cc57-b9eb89092647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: An Nvidia GPU (Graphics Processing Unit) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.  In simpler terms, it's a powerful chip inside your Nvidia laptop that handles the graphics, making games run smoothly, videos look sharp, and creative applications like video editing perform much faster.  Unlike a CPU (Central Processing Unit), which handles general-purpose tasks, a GPU is optimized for parallel processing – handling many calculations simultaneously – which is ideal for graphics rendering and other computationally intensive tasks.  Nvidia GPUs also power many other features beyond just graphics, including AI processing, deep learning, and video encoding/decoding.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-1.5-flash\",\n",
        "    api_key=api_key,\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"history\" , \"question\"],\n",
        "    template=\"Previous history:\\n\\n {history} \\\\nYou are a powerfull assistant in Nvidia laptops. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = template,\n",
        "    memory = memory,\n",
        ")\n",
        "\n",
        "# \"Sabse pehle LLMChain memory=memory ko run karta hai, phir input_variables mein history ke ander memory store karta hai?\"\n",
        "\n",
        "# Haan, yeh process kuch is tarah hota hai:\n",
        "\n",
        "# LLMChain memory ko check karta hai aur jo bhi past conversation hai, usse history variable mein inject karta hai.\n",
        "# Iske baad, PromptTemplate ke input variables (history aur question) ko combine karta hai.\n",
        "# Finally, yeh combined prompt ko llm model ke paas bhejta hai.\n",
        "\n",
        "response1 = chain.run(\"What is Nvidia GPU?\")\n",
        "print(\"Answer:\", response1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho3Cjv3rW01P",
        "outputId": "aab42da0-6dc9-4724-a188-8db7a538fed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: As a powerful assistant in Nvidia laptops, I can explain how the Nvidia GPU interacts with memory:\n",
            "\n",
            "The Nvidia GPU uses several types of memory to perform its tasks:\n",
            "\n",
            "* **GDDR (Graphics Double Data Rate) memory:** This is the primary, high-speed memory directly connected to the GPU.  It's used to store textures, models, and other data needed for rendering images and videos.  The GPU accesses GDDR memory extremely quickly, allowing for the smooth, high-resolution visuals you see.  The amount of GDDR memory (e.g., 6GB, 8GB, 12GB) directly impacts the GPU's ability to handle complex tasks and high-resolution content.  Larger amounts of GDDR memory allow for the processing of larger and more detailed graphics.\n",
            "\n",
            "* **System RAM (Random Access Memory):** While the GPU has its own dedicated memory, it can also access system RAM.  This is typically slower than GDDR memory, so it's used less frequently.  The GPU might access system RAM to retrieve data that doesn't need the extremely high speed of GDDR memory or when GDDR memory is full.  The speed and amount of system RAM can indirectly impact GPU performance, particularly in tasks that require significant data transfer between the GPU and the system.\n",
            "\n",
            "* **Cache memory:** The GPU, like a CPU, uses cache memory for incredibly fast access to frequently used data. This significantly speeds up rendering by reducing the time it takes to access frequently needed information.\n",
            "\n",
            "The interaction involves constant data transfer:  the GPU pulls data from system RAM and/or GDDR memory, processes it, and then writes the results back to GDDR memory for display or further processing.  The efficiency of this data transfer, including the speed of the memory and the memory bus connecting the GPU and memory, is crucial for overall performance.  A faster memory bus and larger amounts of faster memory will improve the GPU’s capabilities and allow it to handle more demanding tasks.\n"
          ]
        }
      ],
      "source": [
        "response2 = chain.run(\"How can it be used with memory?\")\n",
        "print(\"Answer:\", response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcD0w1l3W9WE",
        "outputId": "25feb634-ecdb-4a3c-880d-be6660342698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: An Nvidia GPU is a specialized chip optimized for parallel processing, making it exceptionally fast at handling the many calculations needed for graphics rendering and other computationally intensive tasks like AI processing and video encoding/decoding.  Unlike a CPU which handles general tasks, the GPU excels at tasks that can be broken down into many smaller, simultaneous operations.  This is why it makes games run smoothly, videos look sharp, and creative applications perform much faster.\n",
            "\n",
            "To do this, the Nvidia GPU utilizes several types of memory:\n",
            "\n",
            "* **GDDR (Graphics Double Data Rate) memory:** This is the GPU's primary, high-speed memory.  It's directly connected to the GPU and stores the data crucial for rendering – textures, models, etc.  The amount of GDDR memory (e.g., 6GB, 8GB, 12GB) directly impacts the GPU's ability to handle complex graphics and high resolutions. More GDDR means the GPU can handle more detailed and larger scenes.\n",
            "\n",
            "* **System RAM (Random Access Memory):** The GPU can also access system RAM, but this is slower than GDDR.  It's used less often, mainly for data that doesn't require the extreme speed of GDDR or when GDDR is full.  System RAM's speed and size indirectly affect GPU performance, particularly when a lot of data needs transferring between the GPU and system.\n",
            "\n",
            "* **Cache memory:** Like a CPU, the GPU uses cache memory for extremely fast access to frequently used data. This significantly accelerates rendering by minimizing the time spent retrieving information.\n",
            "\n",
            "The process involves a constant flow of data: The GPU retrieves data from GDDR and/or system RAM, processes it, and then writes the results back to GDDR memory for display or further processing. The speed of this data transfer, including the memory speed and the memory bus connecting the GPU and memory, is critical for overall GPU performance. Faster memory and a faster memory bus translate to a more powerful and responsive GPU.  In essence, the GPU’s power is a combination of its processing capabilities and its efficient interaction with different types of memory to handle the demands of graphically intensive and computationally heavy applications.\n"
          ]
        }
      ],
      "source": [
        "response3 = chain.run(\"Can you explain both answers together?\")\n",
        "print(\"Answer:\", response3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwpMpn6QbpXX",
        "outputId": "f169b51d-a20b-4c5c-dbdd-31b5459557fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: You asked: \"Can you explain both answers together?\"\n"
          ]
        }
      ],
      "source": [
        "response4 = chain.run(\"Which question was i ask\")\n",
        "print(\"Answer:\", response4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psQd51o5dgX-",
        "outputId": "bc6618c5-39fe-4df9-fbbe-6b73ee44c134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).  Think of it as a toolbox filled with components that let you easily connect LLMs to other sources of data and functionality.  Instead of writing all the code from scratch to manage prompts, handle outputs, and interact with external resources like databases or APIs, LangChain provides pre-built modules to streamline this process.\n",
            "\n",
            "Essentially, LangChain helps you build more complex and useful applications beyond simple LLM interactions. This includes features like:\n",
            "\n",
            "* **Chain construction:**  Connecting multiple LLMs or combining LLMs with other components to create complex workflows (e.g., using an LLM to generate a query for a database, then using another LLM to summarize the results).\n",
            "* **Memory:** Giving your application a \"memory\" so it can remember previous interactions and maintain context across multiple turns in a conversation.\n",
            "* **Indexes:**  Enabling LLMs to interact with your own documents or databases efficiently, letting you use your private data to power LLM applications.\n",
            "* **Agents:**  Giving your application the ability to decide which tools or LLMs to use based on the context of the user's request.\n",
            "\n",
            "\n",
            "In short, LangChain is a powerful tool for developers looking to build sophisticated and robust applications leveraging the capabilities of large language models.  It abstracts away many of the complexities involved, allowing you to focus on the core logic of your application.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "# Configuration for the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,  # Changed from \"GOOGLE_API_KEY\" to GOOGLE_API_KEY\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create the PromptTemplate\n",
        "# Changed input_variables to ['history', 'input']\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"history\", \"question\"],\n",
        "    template=\"Previous history:\\n\\n{history}\\nYou are a powerful assistant in Nvidia laptops. Answer the following question:\\n\\n{input}\"\n",
        ")\n",
        "\n",
        "# Create the ConversationChain\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=template,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "# Running the chain\n",
        "response = chain.run(\"What is LangChain?\")\n",
        "print(\"Answer:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Integrate Tools**: Extend the chain to include tools like database queries or APIs.\n",
        "\n",
        "* **Define the Tool**: Ek tool create kiya jata hai jo ek specific kaam, jaise database se data fetch karna ya ek API call karna, karta hai.\n",
        "\n",
        "* **Integrate the Tool**: Us tool ko apne AI model ke saath integrate kiya jata hai. Jab bhi AI model se koi query hoti hai, us tool ka use hota hai aur wo external data ya functionality perform karta hai."
      ],
      "metadata": {
        "id": "OFmuevSc6NQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_google_genai"
      ],
      "metadata": {
        "id": "7K0Y7HFN05J2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import requests\n",
        "from langchain.tools import tool\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "\n",
        "from google.colab import userdata\n",
        "weather_api = userdata.get('weather_api')\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "@tool\n",
        "def calculate_expression(expression: str) -> str:\n",
        "  \"\"\"\n",
        "    This tool evaluates a mathematical expression.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    result = eval(expression)\n",
        "    return f\"The result of {expression} is {result}\"\n",
        "  except Exception as e:\n",
        "    return f\"Error in calculation : {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def fetch_weather(city:str):\n",
        "  \"\"\"\n",
        "    This tool fetches the weather for a given city using an API.\n",
        "  \"\"\"\n",
        "  api_key = weather_api\n",
        "  url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    weather_description = data[\"weather\"][0][\"description\"]\n",
        "    temperature = data[\"main\"][\"temp\"]\n",
        "    return f\"The weather in {city} is {weather_description} with a temperature of {temperature} °C.\"\n",
        "  else:\n",
        "    return f\"Error fetching weather data: {city}\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"chat_history\"],\n",
        "    template='''Previous Conversation:\\n{chat_history}\\n\\n\n",
        "    You are a knowledgeable assistant specializing in web development, weather updates, and mathematical calculations.\n",
        "    \\n\\nUser Input:\\n{input}'''\n",
        ")\n",
        "\n",
        "tools = [calculate_expression, fetch_weather]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    llm = llm,\n",
        "    tools=tools,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# eval() ek Python function hai jo string mein likhi expression ko as code run karta hai.\n",
        "# Iska kaam yeh hai ke agar aap ek string mein mathematical expression dete ho (jaise \"2 + 3 * 4\"),\n",
        "# toh yeh usko calculate karke result de deta hai.\n",
        "\n",
        "# return_messages(true)\n",
        "# True: Jab messages ka individual structure chahiye, jaise {'user': 'question', 'assistant': 'answer'}.\n",
        "# False: Jab sirf plain-text history chahiye.\n"
      ],
      "metadata": {
        "id": "izWvtEgm7Ug2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(f\"Input: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "      break\n",
        "    else:\n",
        "      response = agent.run(user_input)\n",
        "      print(f\"Answer: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe4FRavwJ5ee",
        "outputId": "f2705963-1671-4554-ebfd-fd57ad6d12dc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: whats the weather of gujar khan\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: fetch_weather\n",
            "Action Input: Gujar Khan\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mError fetching weather data: Gujar Khan\n",
            "```\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: I'm sorry, I couldn't find weather information for Gujar Khan.  The weather API I'm using may not have data for that specific location.  Could you try a slightly larger city or region nearby?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: I'm sorry, I couldn't find weather information for Gujar Khan.  The weather API I'm using may not have data for that specific location.  Could you try a slightly larger city or region nearby?\n",
            "Input: weather of pakistan\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: fetch_weather\n",
            "Action Input: Pakistan\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mThe weather in Pakistan is clear sky with a temperature of 10.11 °C.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: I'm sorry, I can only provide a general weather overview for Pakistan.  The weather varies significantly across the country.  To get a more accurate forecast, please specify a city or region in Pakistan.\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: I'm sorry, I can only provide a general weather overview for Pakistan.  The weather varies significantly across the country.  To get a more accurate forecast, please specify a city or region in Pakistan.\n",
            "```\n",
            "Input: web development\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: Web development is a broad field encompassing the creation and maintenance of websites and web applications.  It involves various aspects, including:\n",
            "\n",
            "* **Frontend Development:** This focuses on the user interface (UI) and user experience (UX) of a website.  Technologies used often include HTML, CSS, and JavaScript, along with frameworks like React, Angular, or Vue.js.  Frontend developers are responsible for making the website visually appealing and easy to navigate.\n",
            "\n",
            "* **Backend Development:** This deals with the server-side logic and database interactions.  Popular languages include Python, Java, PHP, Node.js (JavaScript), Ruby, and others.  Backend developers handle data processing, security, and the overall functionality of the website.\n",
            "\n",
            "* **Database Management:**  This involves designing, implementing, and maintaining databases to store and retrieve website data.  Common database systems include MySQL, PostgreSQL, MongoDB, and others.\n",
            "\n",
            "* **Full-Stack Development:**  Full-stack developers possess skills in both frontend and backend development, allowing them to work on all aspects of a web application.\n",
            "\n",
            "* **DevOps:** This involves the collaboration between development and operations teams to automate and streamline the software development lifecycle.\n",
            "\n",
            "To learn more about web development, you might want to specify an area that interests you, such as a particular programming language, framework, or aspect of web development (e.g., \"What are the best resources for learning React?\" or \"How do I set up a local development environment for Python web development?\").\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Web development is a broad field encompassing the creation and maintenance of websites and web applications.  It involves various aspects, including:\n",
            "\n",
            "* **Frontend Development:** This focuses on the user interface (UI) and user experience (UX) of a website.  Technologies used often include HTML, CSS, and JavaScript, along with frameworks like React, Angular, or Vue.js.  Frontend developers are responsible for making the website visually appealing and easy to navigate.\n",
            "\n",
            "* **Backend Development:** This deals with the server-side logic and database interactions.  Popular languages include Python, Java, PHP, Node.js (JavaScript), Ruby, and others.  Backend developers handle data processing, security, and the overall functionality of the website.\n",
            "\n",
            "* **Database Management:**  This involves designing, implementing, and maintaining databases to store and retrieve website data.  Common database systems include MySQL, PostgreSQL, MongoDB, and others.\n",
            "\n",
            "* **Full-Stack Development:**  Full-stack developers possess skills in both frontend and backend development, allowing them to work on all aspects of a web application.\n",
            "\n",
            "* **DevOps:** This involves the collaboration between development and operations teams to automate and streamline the software development lifecycle.\n",
            "\n",
            "To learn more about web development, you might want to specify an area that interests you, such as a particular programming language, framework, or aspect of web development (e.g., \"What are the best resources for learning React?\" or \"How do I set up a local development environment for Python web development?\").\n",
            "```\n",
            "Input: claculate percentage of student marks : 560 , total marks : 1100\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: calculate_expression\n",
            "Action Input: (560 / 1100) * 100\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe result of (560 / 1100) * 100 is 50.90909090909091\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: The student's percentage is approximately 50.91%.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The student's percentage is approximately 50.91%.\n",
            "Input: about nvidia \n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: NVIDIA is a multinational technology company specializing in the design and manufacture of graphics processing units (GPUs) and related technologies.  They are a major player in several industries, including:\n",
            "\n",
            "* **Gaming:** NVIDIA GPUs are widely used in gaming PCs and consoles, providing high-performance graphics for immersive gaming experiences.\n",
            "* **Artificial Intelligence (AI):**  NVIDIA's GPUs are crucial for accelerating AI computations, powering machine learning models and deep learning applications in various fields.  Their CUDA platform plays a major role in AI development.\n",
            "* **Data Centers:**  NVIDIA provides high-performance computing solutions for data centers, enabling large-scale data processing and analytics.\n",
            "* **Professional Visualization:**  NVIDIA GPUs are used in professional applications like CAD, digital content creation, and scientific visualization, offering powerful rendering and computation capabilities.\n",
            "* **Automotive:**  NVIDIA is involved in the development of autonomous driving technology, providing hardware and software for self-driving cars.\n",
            "\n",
            "To learn more, you might want to specify an area of interest, such as their specific products (e.g., \"Tell me about the NVIDIA RTX 4090\"), their involvement in AI (e.g., \"What is CUDA?\"), or their work in a specific industry (e.g., \"How does NVIDIA contribute to autonomous driving?\").\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: NVIDIA is a multinational technology company specializing in the design and manufacture of graphics processing units (GPUs) and related technologies.  They are a major player in several industries, including:\n",
            "\n",
            "* **Gaming:** NVIDIA GPUs are widely used in gaming PCs and consoles, providing high-performance graphics for immersive gaming experiences.\n",
            "* **Artificial Intelligence (AI):**  NVIDIA's GPUs are crucial for accelerating AI computations, powering machine learning models and deep learning applications in various fields.  Their CUDA platform plays a major role in AI development.\n",
            "* **Data Centers:**  NVIDIA provides high-performance computing solutions for data centers, enabling large-scale data processing and analytics.\n",
            "* **Professional Visualization:**  NVIDIA GPUs are used in professional applications like CAD, digital content creation, and scientific visualization, offering powerful rendering and computation capabilities.\n",
            "* **Automotive:**  NVIDIA is involved in the development of autonomous driving technology, providing hardware and software for self-driving cars.\n",
            "\n",
            "To learn more, you might want to specify an area of interest, such as their specific products (e.g., \"Tell me about the NVIDIA RTX 4090\"), their involvement in AI (e.g., \"What is CUDA?\"), or their work in a specific industry (e.g., \"How does NVIDIA contribute to autonomous driving?\").\n",
            "```\n",
            "Input: wheather of Rawalpindi\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: fetch_weather\n",
            "Action Input: Rawalpindi, Pakistan\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mThe weather in Rawalpindi, Pakistan is overcast clouds with a temperature of 14.02 °C.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI:The weather in Rawalpindi, Pakistan is currently overcast with a temperature of 14.02 degrees Celsius.\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The weather in Rawalpindi, Pakistan is currently overcast with a temperature of 14.02 degrees Celsius.\n",
            "```\n",
            "Input: weather of Gujar Khan\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```tool_code\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: fetch_weather\n",
            "Action Input: Gujar Khan, Pakistan\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mThe weather in Gujar Khan, Pakistan is broken clouds with a temperature of 13.82 °C.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: The weather in Gujar Khan, Pakistan is currently broken clouds with a temperature of 13.82 °C.\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The weather in Gujar Khan, Pakistan is currently broken clouds with a temperature of 13.82 °C.\n",
            "```\n",
            "Input: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXChHu1OZFSf"
      },
      "source": [
        "# Project1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnqeCQVqZJzs",
        "outputId": "9dd0e080-2ea4-4204-a7c1-6a99840dec7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: mercedes benz\n",
            "Answer: Thank you for contacting us.  While I appreciate your inquiry, my expertise lies solely in web development.  Therefore, I'm unable to assist you with questions regarding Mercedes Benz.  If you have any questions related to website design, development, hosting, or any other web development topic, please don't hesitate to ask.\n",
            "Input: what is web hosting\n",
            "Answer: Web hosting is a service that provides the necessary infrastructure to make your website accessible on the internet.  Think of it like renting space on a powerful computer that's always connected to the internet.  This \"space\" stores your website's files (images, text, code, etc.) and makes them available to anyone who types your website's address into their browser.\n",
            "\n",
            "Different types of web hosting exist, each with varying levels of resources and control, such as shared hosting (multiple websites share server resources), VPS (virtual private server) hosting, and dedicated hosting (you have an entire server to yourself).  Choosing the right type depends on your website's needs and traffic volume.\n",
            "Input: Nvidia powerfull gpu\n",
            "Answer: Thank you for contacting us.  While NVIDIA produces powerful GPUs, which are often used in web development contexts (e.g., for rendering, machine learning tasks related to website optimization), your inquiry seems to be focused on the hardware itself rather than its application within web development.  My expertise lies solely in web development.  If you have any questions related to website design, development, hosting, or any other web development topic, please don't hesitate to ask.  I'd be happy to help with those.\n",
            "Input: thanks web assistant nice to meet you\n",
            "Answer: It's nice to meet you too!  How can I help you with your web development needs today?\n",
            "Input: exit\n",
            "Conversation ended.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Configuration for the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=2500,\n",
        ")\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create the PromptTemplate\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"history\" , \"query\"],\n",
        "    template = \"\"\"Previous history:\\n\\n{history}\\n\n",
        "You are a customer support agent specializing in the web development field.\n",
        "Please respond to the following query in a polite and professional manner:\n",
        "{query}\n",
        "If the query is unrelated to web development, politely inform the user that you are here to assist only with web development-related questions.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Create the ConversationChain\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=template,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_input = input(f\"Input: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    else:\n",
        "        # Running the chain\n",
        "        response = chain.run(user_input)\n",
        "        print(f\"Answer: {response}\")\n",
        "\n",
        "print(\"Conversation ended.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP146IIpAZksXMjHvtBcbsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}